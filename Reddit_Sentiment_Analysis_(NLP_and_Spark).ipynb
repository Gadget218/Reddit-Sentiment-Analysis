{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import asyncpraw\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from textblob import TextBlob\n",
        "from wordcloud import WordCloud\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Access the variables from the environment\n",
        "CLIENT_ID = os.getenv(\"REDDIT_CLIENT_ID\")\n",
        "CLIENT_SECRET = os.getenv(\"REDDIT_CLIENT_SECRET\")\n",
        "USER_AGENT = os.getenv(\"REDDIT_USER_AGENT\")\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SentimentAnalysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Initialize Reddit client using asyncpraw\n",
        "reddit = asyncpraw.Reddit(\n",
        "    client_id=CLIENT_ID,\n",
        "    client_secret=CLIENT_SECRET,\n",
        "    user_agent=USER_AGENT\n",
        ")\n",
        "\n",
        "# Function to fetch data from Reddit\n",
        "async def fetch_data(subreddit_name):\n",
        "    subreddit = await reddit.subreddit(subreddit_name)\n",
        "    print(f\"Fetching data from {subreddit_name}...\")\n",
        "\n",
        "    # Retrieve the latest 10 hot posts\n",
        "    hot_posts = subreddit.hot(limit=10)\n",
        "\n",
        "    # Stream through the posts and process them\n",
        "    async for post in hot_posts:\n",
        "        print(post.title)\n",
        "        await process_post(post.title)\n",
        "\n",
        "# Function to process the post title and analyze sentiment\n",
        "async def process_post(post_title):\n",
        "    # Remove non-alphabetical characters and tokenize the text\n",
        "    text = re.sub('[^a-zA-Z]', ' ', post_title)\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "\n",
        "    words = \" \".join(tokens)\n",
        "\n",
        "    # Perform sentiment analysis with TextBlob\n",
        "    blob = TextBlob(words)\n",
        "    sentiment = blob.sentiment.polarity\n",
        "    print(f\"Sentiment polarity for post '{post_title}': {sentiment}\")\n",
        "\n",
        "    # You can now create a word cloud if needed\n",
        "    wcloud = WordCloud(width=800, height=600, random_state=101).generate(words)\n",
        "    wcloud.to_file(\"sentiment_analysis_wordcloud.png\")\n",
        "\n",
        "    # Process data with Spark\n",
        "    # Create a DataFrame from the processed post data\n",
        "    data = [(post_title, sentiment)]\n",
        "    columns = [\"title\", \"sentiment\"]\n",
        "    df = spark.createDataFrame(data, columns)\n",
        "\n",
        "    # Perform a simple Spark transformation: Filter posts with positive sentiment\n",
        "    positive_posts = df.filter(col(\"sentiment\") > 0)\n",
        "    positive_posts.show()\n",
        "\n",
        "# Function to run the streaming data fetch\n",
        "async def stream_reddit_data(subreddit_name):\n",
        "    await fetch_data(subreddit_name)\n",
        "\n",
        "# Run the async function in the current event loop\n",
        "if __name__ == \"__main__\":\n",
        "    subreddit_name = \"worldnews\"  # You can change this to any subreddit you like\n",
        "    # Use await instead of asyncio.run()\n",
        "    await stream_reddit_data(subreddit_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70R_00A6sV3h",
        "outputId": "b8e5f44f-ce9e-44ff-8141-88d08b5d5f0b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching data from worldnews...\n",
            "/r/WorldNews Live Thread: Russian Invasion of Ukraine Day 1041, Part 1 (Thread #1188)\n",
            "Sentiment polarity for post '/r/WorldNews Live Thread: Russian Invasion of Ukraine Day 1041, Part 1 (Thread #1188)': 0.06818181818181818\n",
            "+--------------------+-------------------+\n",
            "|               title|          sentiment|\n",
            "+--------------------+-------------------+\n",
            "|/r/WorldNews Live...|0.06818181818181818|\n",
            "+--------------------+-------------------+\n",
            "\n",
            "/r/WorldNews Live Thread: Israel at War (Thread #83)\n",
            "Sentiment polarity for post '/r/WorldNews Live Thread: Israel at War (Thread #83)': 0.13636363636363635\n",
            "+--------------------+-------------------+\n",
            "|               title|          sentiment|\n",
            "+--------------------+-------------------+\n",
            "|/r/WorldNews Live...|0.13636363636363635|\n",
            "+--------------------+-------------------+\n",
            "\n",
            "Ukraine's Defence Intelligence reports historic strike: naval drone destroys Russian Mi-8 helicopter in Crimea\n",
            "Sentiment polarity for post 'Ukraine's Defence Intelligence reports historic strike: naval drone destroys Russian Mi-8 helicopter in Crimea': 0.0\n",
            "+-----+---------+\n",
            "|title|sentiment|\n",
            "+-----+---------+\n",
            "+-----+---------+\n",
            "\n",
            "BBC News - US Treasury says it was hacked by China\n",
            "Sentiment polarity for post 'BBC News - US Treasury says it was hacked by China': 0.0\n",
            "+-----+---------+\n",
            "|title|sentiment|\n",
            "+-----+---------+\n",
            "+-----+---------+\n",
            "\n",
            "Taiwan reportedly building hypersonic missiles that can hit north of Beijing\n",
            "Sentiment polarity for post 'Taiwan reportedly building hypersonic missiles that can hit north of Beijing': 0.0\n",
            "+-----+---------+\n",
            "|title|sentiment|\n",
            "+-----+---------+\n",
            "+-----+---------+\n",
            "\n",
            "Support for Trudeau Liberals fall to historic lows, rivalling party's worst defeat\n",
            "Sentiment polarity for post 'Support for Trudeau Liberals fall to historic lows, rivalling party's worst defeat': -0.5\n",
            "+-----+---------+\n",
            "|title|sentiment|\n",
            "+-----+---------+\n",
            "+-----+---------+\n",
            "\n",
            "Fire rips through Russian oil depot deep behind front line\n",
            "Sentiment polarity for post 'Fire rips through Russian oil depot deep behind front line': -0.13333333333333333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x78e32537e800>\n",
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x78e32997d6c0>\n",
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x78e32997d540>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------+\n",
            "|title|sentiment|\n",
            "+-----+---------+\n",
            "+-----+---------+\n",
            "\n",
            "WHO urges China to share Covid origins data, five years on from pandemic’s emergence\n",
            "Sentiment polarity for post 'WHO urges China to share Covid origins data, five years on from pandemic’s emergence': 0.0\n",
            "+-----+---------+\n",
            "|title|sentiment|\n",
            "+-----+---------+\n",
            "+-----+---------+\n",
            "\n",
            "Sixty-mile drag mark found near damaged Baltic Sea cable, says Finland \n",
            "Sentiment polarity for post 'Sixty-mile drag mark found near damaged Baltic Sea cable, says Finland ': 0.0\n",
            "+-----+---------+\n",
            "|title|sentiment|\n",
            "+-----+---------+\n",
            "+-----+---------+\n",
            "\n",
            "500 tonnes of Ukrainian grain to arrive in Syria on 31 December as part of humanitarian programme\n",
            "Sentiment polarity for post '500 tonnes of Ukrainian grain to arrive in Syria on 31 December as part of humanitarian programme': 0.0\n",
            "+-----+---------+\n",
            "|title|sentiment|\n",
            "+-----+---------+\n",
            "+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "em6kaRmTz5fT",
        "outputId": "5e45c50c-844c-4006-8618-6153a79b95b5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AW6fc_YC7vZD",
        "outputId": "1fde6f8a-13b9-46f4-e344-6f807ed1861a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b7uS0NCH717m"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}